1. 线程没有办法干预其他线程线程对象的生命周期
- 只能线程的target对象自己定义好中断点和中断机制
- 执行join的上下文线程可以同步等待被join的线程
- 但daemon就完全没法干预了，只有与虚拟机同步生命周期一个限制
2. Event, Condition, Semaphore都提供了同步功能(阻塞+挂起两个功能的聚合)，Condition和Semaphore还支持了加锁acquire
3. 加锁Lock和RLock，Queue的通信作用和阻塞作用(自己实现了线程安全)
4. 平行空间thread.local()
5. 死锁两个条件：一线程多锁和顺序循环
6. 线程池的作用


```
#! usr/bin/python3
import time
from queue import Queue
from threading import Thread, Event, Condition, Semaphore, RLock, Lock
from multiprocessing import Process

def splite_out(func):
    def wrap(*args):
        name = func.__name__
        print(name.center(20, "-"))
        func()
        print()
    return wrap

class ThreadIns1(object):
    def __init__(self, key):
        self.m_key = key
        self.m_Stop = False

    def run(self, *args):
        if args:
            evt = args[0]
            evt.set()
        while not self.m_Stop:
            print("Im thread " + self.m_key)
            pass

    def stop(self):
        self.m_Stop = True

class ThreadIns2(Thread):
    def __init__(self, *args, **kw):
        super(ThreadIns2, self).__init__(*args, **kw)
        self.m_key = ""
        self.m_Stop = False

    def name(self, key):
        self.m_key = key

    def run(self):
        while not self.m_Stop:
            print("Im thread " + self.m_key)
            pass

    def stop(self):
        self.m_Stop = True


@splite_out
def p1():
    # 线程没有办法干预其他线程线程对象的生命周期
    # 只能线程的target对象自己定义好中断点和中断机制

    # 执行join的上下文线程可以同步等待被join的线程
    # 但daemon就完全没法干预了，只有与虚拟机同步生命周期一个限制
    _t1 = ThreadIns1("type1")
    t1 = Thread(target=_t1.run)
    t1.start()
    print("sleehreadppp", time.time())
    time.sleep(5)
    print("backkk", time.time())
    _t1.stop()

    t2 = ThreadIns2()
    t2.name("type2")
    t2.start()

    _t3 = ThreadIns1("type3")
    t3 = Thread(target=_t3.run)
    t3.setDaemon(True)
    t3.start()

    _t5 = ThreadIns1("process5")
    t5 = Process(target=_t5.run)
    t5.start()

    _t4 = ThreadIns1("type4")
    t4 = Thread(target=_t4.run)
    t4.start()
    print("befor join")
    t4.join()
    print("never exec here")

class ConditionWaiter(object):
    def __init__(self):
        self.con = Condition()

    def notify(self):
        with self.con:
            self.con.notify_all()

    def wait(self, name):
        while True:
            with self.con:
                print("start wait", name)
                self.con.wait()
                print("after awakeeee", name)

@splite_out
def p2():
    evt = Event()
    def thread_ins0(e, name):
        print("event set: ", name)
        e.set()
    t = Thread(target=thread_ins0, args=(evt, "e1"))
    t.start()
    print("never befor event set")

    c = ConditionWaiter()
    def thread_ins(cond, name):
        cond.wait(name)
    t1 = Thread(target=thread_ins, args=(c, "t1"))
    t2 = Thread(target=thread_ins, args=(c, "t2"))
    t1.start()
    t2.start()
    n = 3
    while n:
        n -= 1
        time.sleep(3)
        c.notify()

    sema = Semaphore(0)
    def thread_ins2(s, name):
        while True:
            print("sema aquire: ", name)
            s.acquire()
            print("waake: ", name)
    t3 = Thread(target=thread_ins2, args=(sema, "t3"))
    t4 = Thread(target=thread_ins2, args=(sema, "t4"))
    t3.start()
    t4.start()
    n = 3
    while n:
        n -= 1
        time.sleep(3)
        sema.release()

@splite_out
def p3():
    # 阻塞跟加锁是没关系的
    # 只不过condition，semaphore都同时实现了两个功能
    # 多线程也常要同时处理这两个问题

    # queue既可以传输数据，也可以用于同步多线程
    # 同时queue本身是线程安全的不需要额外加锁，其他容器就需要自己保证安全
    # 比如也可以用condition的acquire和自己的容器来保证数据安全
    import random
    def consume(q):
        while True:
            data, e = q.get()
            e and e.set()  # 锁定produce的生产速度用的，但这里消费比较快

            print("get data: ", data)
            q.task_done()
            time.sleep(0.5)
    def produce(q):
        while True:
            data = random.randint(0, 10)
            e = Event()
            print("produce data: ", data)
            q.put((data, e), block=False)

            e and e.wait()
            time.sleep(1)
    qu = Queue()
    qu.put((-1, None))
    qu.put((-2, None))
    Thread(target=consume, args=(qu, )).start()
    Thread(target=produce, args=(qu, )).start()
    qu.join()  # queue提供的同步，等待第一次队列空，且只有一次
    print("queue empty")


@splite_out
def p4():
    class A(object):
        rlock = RLock()
        def __init__(self):
            self.res = 0
            self.lock = Lock()
        def inc(self):
            while True:
                with self.lock:
                    self.res += 1
                    print("inc: ", self.res)
        def dec(self):
            while True:
                with self.lock:
                    self.res -= 1
                    print("dec: ", self.res)

        # 可重入，对象锁会死锁(连锁自己都排斥)，但这里会无线递归
        # def inc1(self):
        #     with self.rlock:
        #         self.res += 1
        #         print("inc: ", self.res)
        #         self.dec1()
        # def dec1(self):
        #     with self.rlock:
        #         self.res -= 1
        #         print("dec: ", self.res)
        #         self.inc1()

    a = A()
    Thread(target=a.inc).start()
    Thread(target=a.dec).start()


def p5():
    import threading
    from contextlib import contextmanager

    @contextmanager
    def my_acq(*locks):
        _locks = sorted(locks, key=lambda l: id(l))
        _local = threading.local()
        acqs = getattr(_local, "acquired", [])
        if acqs and filter(lambda l: max(acqs)>l, _locks):
            raise Exception("dead lock risk")

        acqs.extend(_locks)
        _local.acquired = acqs

        try:
            for l in _locks:
                print("acquire lock: ", id(l))
                l.acquire()
            yield
        finally:
            for l in reversed(_locks):
                l.release()
            del acqs[-len(acqs):]

    lock1, lock2 = Lock(), Lock()
    def thread_ins1():
        print("run th1")
        with my_acq(lock1, lock2):
            print("ins 1")
        print("end th1")
    def thread_ins2():
        print("run th2")
        with my_acq(lock2, lock1):
            print("ins2")
        print("end ht2")
    Thread(target=thread_ins1).start()
    Thread(target=thread_ins2).start()

def p6():
    import threading
    class Instance(object):
        def __init__(self):
            self.local = threading.local()  # 如果在多线程中操作则效果是平行的

        def parall(self, val, name=""):
            while True:
                if not hasattr(self.local, "parall_Val"):
                    self.local.parall_Val = 0
                self.local.parall_Val += val
                print(name, self.local.parall_Val)
                time.sleep(1)

    ins = Instance()
    Thread(target=ins.parall, args=(1, "t1")).start()
    Thread(target=ins.parall, args=(-1, "t2")).start()

def p7():
    from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
    # 比起手动多线程，线程执行完有返回feature对象，里面携带返回结果
    def thread_ins(name, dur):
        while dur:
            dur -= 1
            time.sleep(1)
            print(name, dur)
        return name

    pool = ThreadPoolExecutor(128)
    ret1 = pool.submit(thread_ins, "t1", 3)
    ret2 = pool.submit(thread_ins, "t2", 6)
    ret3 = pool.submit(thread_ins, "t3", 1)
    print("start pool")
    print("not block result: ", ret3.add_done_callback(lambda future: print("t3 finish cb", future.result())))
    print("test block")
    print("block result: ", ret2.result())
    print("block 1")
    print("block result2: ", ret1.result())  # 下边的result也会被上边的阻塞

def thread_ins(e):
    return e ** 2
def thread_ins2(l):
    return list(map(thread_ins, l))
def p8():
    from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
    # 多开解释器，在解释器之间传递对象的
    # 限制比较多，比如只能用简单函数不能是闭包匿名
    # 传递的数据必须是课pickle的，内部函数居然还不行
    # AttributeError: Can't pickle local object 'p8.<locals>.thread_ins'
    table = [[_ for _ in range(1000)] for _ in range(4)]
    interpreterPool = ProcessPoolExecutor(4)
    t1 = time.time()
    ret1 = interpreterPool.map(thread_ins, table[0])
    ret2 = interpreterPool.map(thread_ins, table[1])
    ret3 = interpreterPool.map(thread_ins, table[2])
    ret4 = interpreterPool.map(thread_ins, table[3])
    t2 = time.time()
    print(ret1.result()[999])
    print("process finsh during: ", t2 - t1)

    threadPool = ThreadPoolExecutor(4)
    t1 = time.time()
    ret5 = threadPool.submit(thread_ins2, table[0])
    ret6 = threadPool.submit(thread_ins2, table[1])
    ret7 = threadPool.submit(thread_ins2, table[2])
    ret8 = threadPool.submit(thread_ins2, table[3])
    t2 = time.time()
    print(ret5.result()[999])
    print("thread finsh during: ", t2 - t1)



if __name__ == "__main__":
    # p1()
    # p2()
    # p3()
    # p4()
    # p5()
    # p6()
    # p7()
    p8()
```
